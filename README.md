# Real-time-gesture-and-emotion-detection
Всем привет!
Рад представить вам мой проект, в котором я обучил YOLOv8 распознавать жесты и эмоции на видео.

### Стек:
* ultralytics (YOLO v8)
* cv2
* Label-Studio
* os, shutil 

### Структура репозитория:

* **app.py** - скрипт с запуском обученной нейросетки и веб-камеры, стоящей у вас по умолчанию
* **best.pt** - веса обученной модели 
* **train.py** - скрипт для обучения. Я обучал нейросеть в Google Colab'е
* **data.yaml** - файл разметки для примера. В нем описана структура папок для обучени оригинально обученной нейросети (yolov8n.pt в моем случае)
  

### Почему эмоции и жесты?
Как известно, в процессе общения люди обмениваются не только вербальной информацией (речь), но и невербальной - жестикуляция, позы, мимика. Есть еще интонация, и она также играет важную роль в коммуникации, однако о ней как-нибудь потом. В отдельных случаях, люди используют жестикуляцию как основной способ донести до собеседника свои мысли - например, глухонемые и слабослышащие люди используют жестовый язык. 

В перспективе, детекцию жестов и мимики можно использовать для распознавания речи глухонемых или для анализа «языка тела», например, психологам.

Я не претендую на научную новизну своего эксперимента, а подобные работы уже давно ведутся более опытными исследователями. Однако, мне стала интересна реализация своего собственного детектора жестов и эмоций посредством нейросети YOLO версии 8. 

Весь код, а также саму модель я публикую в открытом доступе для всех желающих, ведь возможно для кого-то он станет отправной точкой для своих исследований или же поможет начать использовать YOLO для своих задач!

### Данные 
Для начала было решено ограничиться лишь тремя эмоциями (улыбающийся, спокойный и грустный) и четырьми жестами (большой палец вверх, палец вниз, приветствие и вулканский салют Спока)
Обучающий материал мы с друзьями записали на видео, а частично взяли из интернета. Видео разделили покадрово, взяв каждый пятый кадр.
А затем разметили все 849 изображений в Label-Studio.

<img src="https://github.com/user-attachments/assets/fd360e5a-3a0f-46fe-8796-00362ebf5208" width=50% height=50% />

Данные я разделил на тренировочную, валидационную и тестовую части датасета.

### Обучение и результаты
Для обучения была взята самая маленькая модель восьмой версии - yolov8n.
Само обучение проходило в Google Colab'е на T4 GPU, early stopping сработал на 230-й эпохе.

<img src="https://github.com/user-attachments/assets/f9652445-755e-4cff-b666-b6b57875be31" width=50% height=50%/>

<img src="https://github.com/user-attachments/assets/fe0d0d91-027f-4219-a65a-e56710151867" width=50% height=50% />

![image](https://github.com/user-attachments/assets/33add1fe-5aee-4405-8c3b-feb01bb76b5f)


Наиболее весомой метрикой для анализа результатов являются mAP50 и mAP50-95. Они используются для определения точности модели с разными порогами IoU (intersection over union - отношение площади пересечения предсказанного bounding box'а к объединенной площади предсказанного и истинного bounding box'ов). С учетом небольшого количества обучающих данных, использования самой компактной модели yolov8 метрики мне видятся неплохими

**Детекция на видео**

![output (2)](https://github.com/user-attachments/assets/f1c5bc1b-766e-44fc-8c4a-49e857851374)


### Выводы:

Конечно же, модель на данном этапе не сможет провести тонкую грань между каждой эмоциональной маской в разных степенях ее проявления у каждого отдельного человека. Определение более сложных эмоций по мимическим движениям требует более глубокого недели детекция подхода.

Для первого захода в область детекции эмоций и жестов я считаю свой проект успешным. Наблюдаются проблемы при детекции при быстром движении, плохом освещении. Иногда происходят ложные срабатывания. 
В перспективе интересно будет посмотреть на поведение модели при добавлении новых жестов и эмоций, а также перехода от задачи детекции к сегментации и трекинга точек лица и ладоней (разметка для которых займет сильно больше времени, чем при рисовании bounding box'ов).

### Источники знаний:

[Официальная документация YOLO](https://docs.ultralytics.com/)

[Статья на Хабре](https://habr.com/ru/articles/821971/)


